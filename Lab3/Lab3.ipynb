{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eric Johnson and Quincy Schurr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we will be using for Lab 3 was obtained from a Kaggle competition called Bago of Words Meets Bag of Popcorn. The dataset can be found here https://www.kaggle.com/c/word2vec-nlp-tutorial/data. \n",
    "\n",
    "The dataset is comprised of 3 files, the test dataset, the unlabeled training dataset, and the labeled training dataset. For this lab we will be using the test and labeled training datasets. The test dataset has 25,000 records that contain an id and then a review. The labeled training dataset contains 25,000 records that include an id, a sentiment score, and a review. The sentiment score is a binary value where 1 is a positive review, given if the rating on IMDB was greater than or equal to 7, and 0 is a negative review, given if the rating on IMDB was less than 5.\n",
    "\n",
    "The purpose of this dataset is understanding sentiment analysis through deep learning architectures. This means that the machine is learning how interpret the meaning behind an expression by learning how language is constructed and then trying to piece together patterns to determine whether an expression is positive or negative in meaning. The dataset was made public to users of Kaggle so that they could learn how to use deep learning architectures in order to classify or predict what people mean when they say certain things or predict what they could say next based on previous things they have said. \n",
    "\n",
    "For this lab we will be classifying the movie reviews as positive or negative based on the sentiment analysis. Since the dataset was already split into a test and training set, we do not have to preprocess the data in order to split it up. We will be choosing two deep learning architectures to run our data through and will be testing the accuracy of each of the architectures. We will also be tuning the hyperparameters to try to obtain the best accruacy for each architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define all imports needed for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, grid_search\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first looked at the data, we brought in both the training and testing data and started using it for analysis. After a while we realized that we could not use an accuracy metric with the dataset we chose because the test data did not have a sentiment value included. Testing accuracy is the best measure with this dataset becuase we are classifying the movie reviews into two categories and we were either correct or incorrect. Using false positives or negatives would not give us a conclusion that would be helpful in determining if this architecture could be used to help with deep learning problems in the future.\n",
    "\n",
    "For this reason, we decided to just use the training data and to split the data into 70% training and 30% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train1'\n",
    "train2_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train2'\n",
    "train1 = pd.read_csv(train1_url, delimiter='\\t')\n",
    "train2 = pd.read_csv(train2_url, delimiter='\\t')\n",
    "train2.columns = ['id', 'sentiment', 'review']\n",
    "X = train1.append(train2, ignore_index=True)\n",
    "X.rename(columns={'id' : 'id_'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['review'].replace(regex=True,inplace=True,to_replace=r'<br \\/>',value=r'')\n",
    "X['review'].replace(regex=True,inplace=True,to_replace=r'[.,\\\\/|#!$%\\^&\\*;:{}=\\-_\\'~()\\?\"]',value=r'')\n",
    "y = X['sentiment']\n",
    "#print(np.min(train_data.review.str.len()))\n",
    "#print(np.min(test_data.review.str.len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_ = tf.contrib.layers.sparse_column_with_hash_bucket(\"id\", hash_bucket_size=25000)\\nreview = tf.contrib.layers.sparse_column_with_hash_bucket(\"review\", hash_bucket_size=250000)\\nsentiment = tf.contrib.layers.sparse_column_with_keys(column_name=\"sentiment\", keys=[0, 1])\\n#review = tf.contrib.layers.real_valued_column(\"review\")\\n\\ndeep_columns = [\\n  tf.contrib.layers.embedding_column(sentiment, dimension=8),\\n  id_, review]\\n\\nwide_columns = [\\n  sentiment,\\n  tf.contrib.layers.crossed_column([id_, sentiment], hash_bucket_size=int(1e4)),\\n  tf.contrib.layers.crossed_column([review, sentiment], hash_bucket_size=int(1e4))]\\n\\nimport tempfile\\nmodel_dir = tempfile.mkdtemp()\\nm = tf.contrib.learn.DNNLinearCombinedClassifier(\\n    model_dir=model_dir,\\n    linear_feature_columns=wide_columns,\\n    dnn_feature_columns=deep_columns,\\n    dnn_hidden_units=[100, 50])\\n\\nm.fit(x=X_train, y=y_train, steps=2000)\\n\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=X_test,\\n                                     y=y_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try new type of embedding - found here \n",
    "#(https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html#tensorflow-wide-deep-learning-tutorial)\n",
    "#categorical base columns\n",
    "'''id_ = tf.contrib.layers.sparse_column_with_hash_bucket(\"id\", hash_bucket_size=25000)\n",
    "review = tf.contrib.layers.sparse_column_with_hash_bucket(\"review\", hash_bucket_size=250000)\n",
    "sentiment = tf.contrib.layers.sparse_column_with_keys(column_name=\"sentiment\", keys=[0, 1])\n",
    "#review = tf.contrib.layers.real_valued_column(\"review\")\n",
    "\n",
    "deep_columns = [\n",
    "  tf.contrib.layers.embedding_column(sentiment, dimension=8),\n",
    "  id_, review]\n",
    "\n",
    "wide_columns = [\n",
    "  sentiment,\n",
    "  tf.contrib.layers.crossed_column([id_, sentiment], hash_bucket_size=int(1e4)),\n",
    "  tf.contrib.layers.crossed_column([review, sentiment], hash_bucket_size=int(1e4))]\n",
    "\n",
    "import tempfile\n",
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])\n",
    "\n",
    "m.fit(x=X_train, y=y_train, steps=2000)\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x=X_test,\n",
    "                                     y=y_test)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Create embedding of all the words in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 157132\n",
      "(17498, 130458)\n",
      "(7500, 73041)\n"
     ]
    }
   ],
   "source": [
    "# Try tweaking these numbers\n",
    "MAX_DOCUMENT_LENGTH = 30\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "#get unique words out of X\n",
    "#Step1 - Vocab Processor\n",
    "X_train = X_train['review']\n",
    "X_test = X_test['review']\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(X_train.append(X_test))\n",
    "X_train = np.array(list(vocab_processor.transform(X_train))).astype('float32')\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "print('Total words: %d' % len(vocab_processor.vocabulary_))\n",
    "\n",
    "#Step 2, one hot encoding\n",
    "le = preprocessing.OneHotEncoder(n_values='auto', categorical_features='all', sparse=True)\n",
    "X_train = le.fit_transform(X_train)\n",
    "X_test = le.fit_transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17498, 82)\n",
      "(7500, 67)\n"
     ]
    }
   ],
   "source": [
    "#step 3 - try embedding\n",
    "rf = ensemble.RandomTreesEmbedding(n_estimators=10, max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, max_leaf_nodes=None, \n",
    "                                   min_impurity_split=1e-07, sparse_output=True, n_jobs=1)\n",
    "X_train = rf.fit_transform(X_train)\n",
    "X_test = rf.fit_transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2k/70cnxvpx2hzd9rsj14_dd13w0000gn/T/tmpzbdcegqe\n",
      "WARNING:tensorflow:Using default config.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Tensor(\"input:0\", shape=(?, 82), dtype=float64)\n",
      "Tensor(\"Reshape:0\", shape=(?, 30, 100, 1), dtype=float64)\n",
      "Through conv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataType float64 for attr 'T' not in list of allowed values: float32, float16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-33254e9d9988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Copy TensorFlow Architecture from \\n#   Deep MNIST for experts\\n#   https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html\\n#which we then copied from Notebook 14 from in class\\n\\nimport tensorflow as tf\\nfrom tensorflow.contrib import learn\\nfrom tensorflow.contrib import layers\\n\\ndef conv_model(X, y):\\n    print('===============================')\\n    print(X)\\n    # get in format expected by conv2d\\n    # (batch_size,width, height,color_channels)\\n    # since our images are gray scale and 28x28 pixels\\n    #   we define the last three elements as 28x28x1\\n    # we don't know the batch size, so just let it \\n    # figure that out from the input data (-1 designation)\\n    # height and width of images\\n    features = tf.reshape(X, [-1, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE, 1])\\n    print(features)\\n    \\n    # create 32 filters of 5x5 size \\n    n_out = 32\\n    kernel = [5,5]\\n    features = layers.conv2d(inputs=features, \\n                            num_outputs=n_out, \\n                            kernel_size=kernel)\\n    print('Through conv')\\n    # add a bias and pass through relu (for concentrated gradients)\\n    features = tf.nn.relu(layers.bias_add(features))\\n    \\n    # 2x2 max pool to reduce image size to 14x14\\n    kernel = [1, 2, 2, 1]\\n    stride = [1, 2, 2, 1]\\n    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\\n    \\n    # create 64 filters of 5x5 size\\n    n_out = 5\\n    kernel = [3,3]\\n    features = layers.conv2d(inputs=features, \\n                            num_outputs=n_out, \\n                            kernel_size=kernel)\\n    \\n    # add a bias and pass through relu (for concentrated gradients)\\n    features = tf.nn.relu(layers.bias_add(features))\\n    \\n    # 2x2 max pool to reduce image size to 7x7\\n    kernel = [1, 2, 2, 1]\\n    stride = [1, 2, 2, 1]\\n    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\\n    \\n\\n    # make the weights a column vector, 7x7x64 = 3136\\n    features = layers.flatten(features)\\n    print(features)\\n    \\n    # pass through fully connected layer with 1024 hidden neurons, W=3136x1024\\n    features = layers.stack(features, layers.fully_connected, [1024])\\n    \\n    # add bias and pass through relu\\n    features = tf.nn.relu(layers.bias_add(features))\\n    print(features)\\n    \\n    # then make a fully connected layer with bias and sigmoid nonlinearity \\n    #  which... is... just logistic regression with one versus all\\n    pred, loss = learn.models.logistic_regression(features, y)\\n    print(pred)\\n    \\n    print('===============================')\\n    \\n    return pred, loss\\n\\n\\n# Create a classifier, train and predict.\\nclassifier = learn.TensorFlowEstimator(model_fn=conv_model, \\n                                       n_classes=2, steps=20000, \\n                                       learning_rate=0.05, batch_size=30)\\n\\n# this operation can take a little while to complete\\n#   Google says it should take about 30 minutes, but \\n#   my machine took a lot longer...\\nclassifier.fit(X_train, y_train)\\n\\n# now predict the outcome\\n#score = accuracy_score(y_test, classifier.predict(X_test))\\n#print(score)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, steps, monitors, logdir)\u001b[0m\n\u001b[1;32m    255\u001b[0m                       \u001b[0mfeed_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_feeder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                       \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                       monitors=monitors)\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m       \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m       \u001b[0;31m# Add default monitors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, targets, mode)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, targets, mode)\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'class_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m       \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrib_framework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mconv_model\u001b[0;34m(X, y)\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m    848\u001b[0m                                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_max_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   1438\u001b[0m   result = _op_def_lib.apply_op(\"MaxPool\", input=input, ksize=ksize,\n\u001b[1;32m   1439\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   1441\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[0;32m--> 573\u001b[0;31m                                        _Attr(op_def, input_arg.type_attr))\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quincyschurr/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"DataType %s for attr '%s' not in list of allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (dtypes.as_dtype(dtype).name, attr_def.name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataType float64 for attr 'T' not in list of allowed values: float32, float16"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Copy TensorFlow Architecture from \n",
    "#   Deep MNIST for experts\n",
    "#   https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html\n",
    "#which we then copied from Notebook 14 from in class\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "def conv_model(X, y):\n",
    "    print('===============================')\n",
    "    print(X)\n",
    "    # get in format expected by conv2d\n",
    "    # (batch_size,width, height,color_channels)\n",
    "    # since our images are gray scale and 28x28 pixels\n",
    "    #   we define the last three elements as 28x28x1\n",
    "    # we don't know the batch size, so just let it \n",
    "    # figure that out from the input data (-1 designation)\n",
    "    # height and width of images\n",
    "    features = tf.reshape(X, [-1, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE, 1])\n",
    "    print(features)\n",
    "    \n",
    "    # create 32 filters of 5x5 size \n",
    "    n_out = 32\n",
    "    kernel = [5,5]\n",
    "    features = layers.conv2d(inputs=features, \n",
    "                            num_outputs=n_out, \n",
    "                            kernel_size=kernel)\n",
    "    print('Through conv')\n",
    "    # add a bias and pass through relu (for concentrated gradients)\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    \n",
    "    # 2x2 max pool to reduce image size to 14x14\n",
    "    kernel = [1, 2, 2, 1]\n",
    "    stride = [1, 2, 2, 1]\n",
    "    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\n",
    "    \n",
    "    # create 64 filters of 5x5 size\n",
    "    n_out = 5\n",
    "    kernel = [3,3]\n",
    "    features = layers.conv2d(inputs=features, \n",
    "                            num_outputs=n_out, \n",
    "                            kernel_size=kernel)\n",
    "    \n",
    "    # add a bias and pass through relu (for concentrated gradients)\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    \n",
    "    # 2x2 max pool to reduce image size to 7x7\n",
    "    kernel = [1, 2, 2, 1]\n",
    "    stride = [1, 2, 2, 1]\n",
    "    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\n",
    "    \n",
    "\n",
    "    # make the weights a column vector, 7x7x64 = 3136\n",
    "    features = layers.flatten(features)\n",
    "    print(features)\n",
    "    \n",
    "    # pass through fully connected layer with 1024 hidden neurons, W=3136x1024\n",
    "    features = layers.stack(features, layers.fully_connected, [1024])\n",
    "    \n",
    "    # add bias and pass through relu\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    print(features)\n",
    "    \n",
    "    # then make a fully connected layer with bias and sigmoid nonlinearity \n",
    "    #  which... is... just logistic regression with one versus all\n",
    "    pred, loss = learn.models.logistic_regression(features, y)\n",
    "    print(pred)\n",
    "    \n",
    "    print('===============================')\n",
    "    \n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "# Create a classifier, train and predict.\n",
    "classifier = learn.TensorFlowEstimator(model_fn=conv_model, \n",
    "                                       n_classes=2, steps=20000, \n",
    "                                       learning_rate=0.05, batch_size=30)\n",
    "\n",
    "# this operation can take a little while to complete\n",
    "#   Google says it should take about 30 minutes, but \n",
    "#   my machine took a lot longer...\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# now predict the outcome\n",
    "#score = accuracy_score(y_test, classifier.predict(X_test))\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model functions\n",
    "From in-class notebook 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 15, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 15, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper-tune parameters with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic outline of gridsearch. Probably will need to tweak\n",
    "param_grid = {\"steps\": [200,400],\n",
    "               \"learning_rate\": [0.1,0.2]}\n",
    "\n",
    "regressor = learn.TensorFlowDNNRegressor(hidden_units=[10, 10],\n",
    "steps=5000, learning_rate=0.1, batch_size=10)\n",
    "\n",
    "# run grid search\n",
    "gs = grid_search.GridSearchCV(\n",
    "         regressor, param_grid=param_grid, scoring = 'accuracy',\n",
    "         verbose=10, n_jobs=-1,cv=2, fit_params={'steps': [200,400]})\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
