{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eric Johnson and Quincy Schurr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we will be using for Lab 3 was obtained from a Kaggle competition called Bago of Words Meets Bag of Popcorn. The dataset can be found here https://www.kaggle.com/c/word2vec-nlp-tutorial/data. \n",
    "\n",
    "The dataset is comprised of 3 files, the test dataset, the unlabeled training dataset, and the labeled training dataset. For this lab we will be using the test and labeled training datasets. The test dataset has 25,000 records that contain an id and then a review. The labeled training dataset contains 25,000 records that include an id, a sentiment score, and a review. The sentiment score is a binary value where 1 is a positive review, given if the rating on IMDB was greater than or equal to 7, and 0 is a negative review, given if the rating on IMDB was less than 5.\n",
    "\n",
    "The purpose of this dataset is understanding sentiment analysis through deep learning architectures. This means that the machine is learning how interpret the meaning behind an expression by learning how language is constructed and then trying to piece together patterns to determine whether an expression is positive or negative in meaning. The dataset was made public to users of Kaggle so that they could learn how to use deep learning architectures in order to classify or predict what people mean when they say certain things or predict what they could say next based on previous things they have said. \n",
    "\n",
    "For this lab we will be classifying the movie reviews as positive or negative based on the sentiment analysis. Since the dataset was already split into a test and training set, we do not have to preprocess the data in order to split it up. We will be choosing two deep learning architectures to run our data through and will be testing the accuracy of each of the architectures. We will also be tuning the hyperparameters to try to obtain the best accruacy for each architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define all imports needed for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24998 entries, 0 to 24997\n",
      "Data columns (total 3 columns):\n",
      "id           24998 non-null object\n",
      "sentiment    24998 non-null int64\n",
      "review       24998 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test1_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/test1'\n",
    "test2_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/test2'\n",
    "\n",
    "test1 = pd.read_csv(test1_url, delimiter='\\t')\n",
    "test2 = pd.read_csv(test2_url, delimiter='\\t')\n",
    "test2.columns = ['id', 'review']\n",
    "test_data = test1.append(test2, ignore_index=True)\n",
    "\n",
    "\n",
    "train1_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train1'\n",
    "train2_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train2'\n",
    "train1 = pd.read_csv(train1_url, delimiter='\\t')\n",
    "train2 = pd.read_csv(train2_url, delimiter='\\t')\n",
    "train2.columns = ['id', 'sentiment', 'review']\n",
    "train_data = train1.append(train2, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['review'].replace(regex=True,inplace=True,to_replace=r'<br \\/>',value=r'')\n",
    "test_data['review'].replace(regex=True,inplace=True,to_replace=r'[.,\\\\/|#!$%\\^&\\*;:{}=\\-_\\'~()\\?\"]',value=r'')\n",
    "#print(test_data.ix[9].review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Create embedding of all the words in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 127377\n"
     ]
    }
   ],
   "source": [
    "# Try tweaking these numbers\n",
    "MAX_DOCUMENT_LENGTH = 30\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = train_data['review']\n",
    "x_test = test_data['review']\n",
    "x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model functions\n",
    "From in-class notebook 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 15, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 15, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ML]",
   "language": "python",
   "name": "Python [ML]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
