{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eric Johnson and Quincy Schurr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we will be using for Lab 3 was obtained from a Kaggle competition called Bago of Words Meets Bag of Popcorn. The dataset can be found here https://www.kaggle.com/c/word2vec-nlp-tutorial/data. \n",
    "\n",
    "The dataset is comprised of 3 files, the test dataset, the unlabeled training dataset, and the labeled training dataset. For this lab we will be using the test and labeled training datasets. The test dataset has 25,000 records that contain an id and then a review. The labeled training dataset contains 25,000 records that include an id, a sentiment score, and a review. The sentiment score is a binary value where 1 is a positive review, given if the rating on IMDB was greater than or equal to 7, and 0 is a negative review, given if the rating on IMDB was less than 5.\n",
    "\n",
    "The purpose of this dataset is understanding sentiment analysis through deep learning architectures. This means that the machine is learning how interpret the meaning behind an expression by learning how language is constructed and then trying to piece together patterns to determine whether an expression is positive or negative in meaning. The dataset was made public to users of Kaggle so that they could learn how to use deep learning architectures in order to classify or predict what people mean when they say certain things or predict what they could say next based on previous things they have said. \n",
    "\n",
    "For this lab we will be classifying the movie reviews as positive or negative based on the sentiment analysis. Since the dataset was already split into a test and training set, we do not have to preprocess the data in order to split it up. We will be choosing two deep learning architectures to run our data through and will be testing the accuracy of each of the architectures. We will also be tuning the hyperparameters to try to obtain the best accruacy for each architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define all imports needed for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Eric/anaconda/envs/ML/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Eric/anaconda/envs/ML/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, grid_search\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first looked at the data, we brought in both the training and testing data and started using it for analysis. After a while we realized that we could not use an accuracy metric with the dataset we chose because the test data did not have a sentiment value included. Testing accuracy is the best measure with this dataset becuase we are classifying the movie reviews into two categories and we were either correct or incorrect. Using false positives or negatives would not give us a conclusion that would be helpful in determining if this architecture could be used to help with deep learning problems in the future.\n",
    "\n",
    "For this reason, we decided to just use the training data for our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train1'\n",
    "train2_url = 'https://raw.githubusercontent.com/quincyschurr/machineLearning/master/Lab3/train2'\n",
    "train1 = pd.read_csv(train1_url, delimiter='\\t')\n",
    "train2 = pd.read_csv(train2_url, delimiter='\\t')\n",
    "train2.columns = ['id', 'sentiment', 'review']\n",
    "X = train1.append(train2, ignore_index=True)\n",
    "X.rename(columns={'id' : 'id_'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation from the data\n",
    "\n",
    "We decided to remove all punctuation from the data set to just focus on the words. Some of the formatting of the data set also included some left over html so we removed that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['review'].replace(regex=True,inplace=True,to_replace=r'<br \\/>',value=r'')\n",
    "X['review'].replace(regex=True,inplace=True,to_replace=r'[.,\\\\/|#!$%\\^&\\*;:{}=\\-_\\'~()\\?\"]',value=r'')\n",
    "y = X['sentiment']\n",
    "#print(np.min(train_data.review.str.len()))\n",
    "#print(np.min(test_data.review.str.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convolutional Neural Network\n",
    "\n",
    "The first model we will run the dataset through is a convolutional model. We got the idea from a paper published by IBM that can be found here (http://www.aclweb.org/anthology/C14-1008). We then read a bit more on a blog that can be found here (http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/). This blog talked about implementing a CNN in tensorflow for sentiment analysis. We liked the idea and decided to do an implementation of it for our dataset. We could not figure out how to to the embedding using tensorflow, so we used sklearn for most of the following.\n",
    "\n",
    "##### Create embedding of all the words in the datasets\n",
    "\n",
    "In this section we create an embedding of all the words in the dataset to use in our convoltional model. The first step was to run the review attribute from the whole dataset through a vocab processor. This way we found out how many unique words there were in the dataset and also assigned a number to each one of those words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try tweaking these numbers\n",
    "MAX_DOCUMENT_LENGTH = 30\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "#get unique words out of X\n",
    "\n",
    "#Step1 - Vocab Processor\n",
    "X = X['review']\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(X)\n",
    "\n",
    "X= np.array(list(vocab_processor.transform(X))).astype('float32')\n",
    "#X_train = np.array(list(vocab_processor.transform(X_train))).astype('float32')\n",
    "#y_train = np.array(y_train)\n",
    "#X_test = np.array(list(vocab_processor.transform(X_test))).astype('float32')\n",
    "#y_test = np.array(y_test)\n",
    "print('Total words: %d' % len(vocab_processor.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding\n",
    "\n",
    "The next step is to take the number representation to a one hot encoding, creating a sparse matrix representation of all the numbered words. To do this we had to take the maximum number and use that as the basis for an array of the maximum document length to input as the n_values since the input needed to take an array of the ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2, one hot encoding\n",
    "num = np.max(X)\n",
    "num2= list()\n",
    "for i in range(0,30):\n",
    "    num2.append(num)\n",
    "\n",
    "le = preprocessing.OneHotEncoder(n_values=num2, categorical_features='all', sparse=True, handle_unknown='ignore')\n",
    "le.fit(X)\n",
    "X = le.transform(X)\n",
    "\n",
    "#reshape into something with only 30 attributes wide      \n",
    "sparse_words = []\n",
    "for i in range(MAX_DOCUMENT_LENGTH):\n",
    "    sparse_words.append(X[:, num*i:(i+1)*num])\n",
    "\n",
    "orig_len = X.shape[0] #need this for hstack later\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "#create a vertical stacking of all these words so that we can input into embedding\n",
    "\n",
    "a = vstack(sparse_words)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding\n",
    "\n",
    "The next step is to take the one hot encoding and transform it into an embedding so that it can be in the correct format to input into a convoltional model. The Random Tress Embedding transforms a sparse matrix into a multi-dimension sparse matrix so that it takes up less space. An option to make the matrix dense, would be to choose False for the sparse_output parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 3 - try embedding\n",
    "rf = ensemble.RandomTreesEmbedding(n_estimators=10, max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, max_leaf_nodes=None, \n",
    "                                   min_impurity_split=1e-07, sparse_output=True, n_jobs=1)\n",
    "\n",
    "rf.fit(a)\n",
    "X = rf.transform(a).astype('float32') \n",
    "#convert to type float32 because the convolutional network model is expecting a float32, not 64.\n",
    "# X_test = rf.transform(X_test).astype('float32')\n",
    "\n",
    "print(X.shape)\n",
    "#embedding size is shape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "orig_len\n",
    "tmp = []\n",
    "for i in range(MAX_DOCUMENT_LENGTH):\n",
    "    tmp.append(X[i*orig_len:(i+1)*orig_len,:])\n",
    "    \n",
    "b = hstack(tmp)\n",
    "print(b.shape)\n",
    "\n",
    "X = b.todense() #create dense matrix for convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Copy TensorFlow Architecture from \n",
    "#   Deep MNIST for experts\n",
    "#   https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html\n",
    "#which we then copied from Notebook 14 from in class\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "def conv_model(X, y):\n",
    "    print('===============================')\n",
    "    print(X)\n",
    "    # get in format expected by conv2d\n",
    "    # (batch_size,width, height,color_channels)\n",
    "    # since our images are gray scale and 28x28 pixels\n",
    "    #   we define the last three elements as 28x28x1\n",
    "    # we don't know the batch size, so just let it \n",
    "    # figure that out from the input data (-1 designation)\n",
    "    # height and width of images\n",
    "    features = tf.reshape(X, [-1, MAX_DOCUMENT_LENGTH, 60, 1])\n",
    "    print(features)\n",
    "    \n",
    "    # create 32 filters of 5x5 size \n",
    "    n_out = 32\n",
    "    kernel = [5,5]\n",
    "    features = layers.conv2d(inputs=features, \n",
    "                            num_outputs=n_out, \n",
    "                            kernel_size=kernel)\n",
    "    print('Through conv')\n",
    "    # add a bias and pass through relu (for concentrated gradients)\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    \n",
    "    # 2x2 max pool to reduce image size to 14x14\n",
    "    kernel = [1, 2, 2, 1]\n",
    "    stride = [1, 2, 2, 1]\n",
    "    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\n",
    "    \n",
    "    # create 64 filters of 5x5 size\n",
    "    n_out = 64\n",
    "    kernel = [5,5]\n",
    "    features = layers.conv2d(inputs=features, \n",
    "                            num_outputs=n_out, \n",
    "                            kernel_size=kernel)\n",
    "    \n",
    "    # add a bias and pass through relu (for concentrated gradients)\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    \n",
    "    # 2x2 max pool to reduce image size to 7x7\n",
    "    kernel = [1, 2, 2, 1]\n",
    "    stride = [1, 2, 2, 1]\n",
    "    features = tf.nn.max_pool(features, ksize=kernel,strides=stride, padding='SAME')\n",
    "    \n",
    "\n",
    "    # make the weights a column vector, 7x7x64 = 3136\n",
    "    features = layers.flatten(features)\n",
    "    print(features)\n",
    "    \n",
    "    # pass through fully connected layer with 1024 hidden neurons, W=3136x1024\n",
    "    features = layers.stack(features, layers.fully_connected, [1024])\n",
    "    \n",
    "    # add bias and pass through relu\n",
    "    features = tf.nn.relu(layers.bias_add(features))\n",
    "    print(features)\n",
    "    \n",
    "    # then make a fully connected layer with bias and sigmoid nonlinearity \n",
    "    #  which... is... just logistic regression with one versus all\n",
    "    pred, loss = learn.models.logistic_regression(features, y)\n",
    "    print(pred)\n",
    "    \n",
    "    print('===============================')\n",
    "    \n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "# Create a classifier, train and predict.\n",
    "classifier = learn.TensorFlowEstimator(model_fn=conv_model, \n",
    "                                       n_classes=2, steps=20000, \n",
    "                                       learning_rate=0.05, batch_size=30)\n",
    "\n",
    "print ('classifier created')\n",
    "\n",
    "# this operation can take a little while to complete\n",
    "#   Google says it should take about 30 minutes, but \n",
    "#   my machine took a lot longer...\n",
    "classifier.fit(X_train, y_train)\n",
    "print ('fit')\n",
    "\n",
    "# now predict the outcome\n",
    "score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run time for this model took a while, and so the output in included below. We ran the convolutional model three differnt times adjusting the second layer from a 5*5 to a 3*3 and changing the learning rate from 0.05 to 0.01, but each time we obtained about 50% accuracy. 50% accuracy does not mean much in terms of getting a valuable architecture since it is about the same as guessing. \n",
    "\n",
    "We thought that this model may be good to try based on the papers that we read but with the dataset we chose, it did not turn out to be the best model. The reason for this could be that the rating metric used in this dataset was a bit arbitrary, since the dataset chose to ignore the ratings that were given 5-7 stars and many words could be used in both positive and negative reviews. Below is the accuracy given for the convolutional model run with a learning rate of 0.05.\n",
    "\n",
    "\n",
    "<img src = 'CNNOutput.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform embedding and train-test split\n",
    "Based on code from in-class notebook 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 157132\n"
     ]
    }
   ],
   "source": [
    "# We redo the embedding to make sure all data is in the expected for and not tainted by the code above\n",
    "MAX_DOCUMENT_LENGTH = 30\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "X = X['review']\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(X)\n",
    "\n",
    "X= np.array(list(vocab_processor.transform(X)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RNN functions (LSTM and GRU cells are tested)\n",
    "From in-class notebook 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 2, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.1)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model_2(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 2, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model_3(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 2, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.001)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_model(x, y):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 2, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.001)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper-tune parameters with grid search and K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Couldn't get normal grid search to work with Tensorflow so this is the poorly programmed version\n",
    "# Each model has a different learning rate\n",
    "\n",
    "scores = []\n",
    "sub_scores = []\n",
    "\n",
    "for k1 in range(0,2):\n",
    "    # Doing 2-fold validation on the parameters because the model takes 40 minutes to run each time\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train, y_train, test_size=0.30)\n",
    "    classifier = learn.Estimator(model_fn=lstm_model)\n",
    "    classifier.fit(X_train_sub, y_train_sub, steps=200)\n",
    "    y_pred = [p['class'] for p in classifier.predict(X_test_sub, as_iterable=True)]\n",
    "    sub_scores.append(metrics.accuracy_score(y_test_sub, y_pred))\n",
    "    print('Fold done ', k1)\n",
    "\n",
    "scores.append(np.mean(sub_scores))\n",
    "\n",
    "for k1 in range(0,2):\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train, y_train, test_size=0.30)\n",
    "    classifier = learn.Estimator(model_fn=lstm_model_2)\n",
    "    classifier.fit(X_train_sub, y_train_sub, steps=200)\n",
    "    y_pred = [p['class'] for p in classifier.predict(X_test_sub, as_iterable=True)]\n",
    "    sub_scores.append(metrics.accuracy_score(y_test_sub, y_pred))\n",
    "    print('Fold done ', k1)\n",
    "\n",
    "\n",
    "scores.append(np.mean(sub_scores))\n",
    "    \n",
    "for k1 in range(0,2):\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train, y_train, test_size=0.30)\n",
    "    classifier = learn.Estimator(model_fn=lstm_model_3)\n",
    "    classifier.fit(X_train_sub, y_train_sub, steps=200)\n",
    "    y_pred = [p['class'] for p in classifier.predict(X_test_sub, as_iterable=True)]\n",
    "    sub_scores.append(metrics.accuracy_score(y_test_sub, y_pred))\n",
    "    print('Fold done ', k1)\n",
    "\n",
    "scores.append(np.mean(sub_scores))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time constraints of running the grid search, a screen shot of a previous run is shown below.\n",
    "\n",
    "<img src=\"GridSearch.png\">\n",
    "\n",
    "This shows that the best of the three learning rates we looked at is 0.001. We use this in the LSTM model below. We do not have time to also perform this grid search on the GRU model so we will just use the same learning rate as the LSTM. Previous testing has led us to believe that the GRU model does not perform as well as the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/47/fv04w9jj44ddpf782t0qcj4w0000gn/T/tmpqngz4437\n",
      "WARNING:tensorflow:Using default config.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x124a97a20>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x12918eb00>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.701067\n",
      "[[2710 1044]\n",
      " [1198 2548]]\n",
      "CPU times: user 1h 53min 46s, sys: 14min 50s, total: 2h 8min 36s\n",
      "Wall time: 41min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build LSTM model\n",
    "classifier = learn.Estimator(model_fn=lstm_model_3)\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(X_train, y_train, steps=200)\n",
    "y_predicted = [p['class'] for p in classifier.predict(X_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/47/fv04w9jj44ddpf782t0qcj4w0000gn/T/tmpaj3odwup\n",
      "WARNING:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.694400\n",
      "[[2515 1239]\n",
      " [1053 2693]]\n",
      "CPU times: user 1h 29min 28s, sys: 13min 1s, total: 1h 42min 30s\n",
      "Wall time: 32min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build GRU model\n",
    "gru_classifier = learn.Estimator(model_fn=gru_model)\n",
    "\n",
    "# Train and predict\n",
    "gru_classifier.fit(X_train, y_train, steps=200)\n",
    "y_predicted_gru = [p['class'] for p in gru_classifier.predict(X_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted_gru)\n",
    "print('Accuracy: {0:f}'.format(score))\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted_gru)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of our models have very similar accuracy results. These accuracies are close enough that we cannot say that one model out-performs the other. The GRU model trains in roughly 75% the time of the LSTM model so this would make it better for a production version of the system. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
